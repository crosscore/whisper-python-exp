import streamlit as st
import sounddevice as sd
import numpy as np
import wave
from datetime import datetime
import os
import threading
import queue
import whisper
import torch
import tempfile
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
ROOT_DIR = Path(__file__).parent.parent
MODEL_DIR = ROOT_DIR / "model"
AUDIO_DIR = ROOT_DIR / "recorded_audio"

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
MODEL_DIR.mkdir(exist_ok=True)
AUDIO_DIR.mkdir(exist_ok=True)

# éŒ²éŸ³è¨­å®š
SAMPLE_RATE = 44100
CHANNELS = 1
CHUNK_DURATION = 3  # ãƒãƒ£ãƒ³ã‚¯ã®é•·ã•ï¼ˆç§’ï¼‰
CHUNK_SAMPLES = int(SAMPLE_RATE * CHUNK_DURATION)

def download_whisper_model():
    """Whisperãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€æŒ‡å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã™ã‚‹"""
    model_path = MODEL_DIR / "model.pt"
    if not model_path.exists():
        print("Downloading Whisper model...")
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        model = whisper.load_model("base")
        # ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã‚’ä¿å­˜
        torch.save(model.state_dict(), str(model_path))
        print(f"Model saved to {model_path}")
    return model_path

def load_whisper_model():
    """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    model_path = MODEL_DIR / "model.pt"
    if not model_path.exists():
        model_path = download_whisper_model()

    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    model = whisper.load_model("base")
    # ä¿å­˜ã•ã‚ŒãŸé‡ã¿ã‚’èª­ã¿è¾¼ã‚€
    state_dict = torch.load(str(model_path))
    model.load_state_dict(state_dict)
    return model

class AudioRecorder:
    def __init__(self, model):
        self.audio_queue = queue.Queue()
        self.is_recording = False
        self.audio_data = []
        self.recognition_queue = queue.Queue()
        self.recognition_thread = None
        self.current_chunk = []
        self.samples_collected = 0
        self.whisper_model = model  # ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¨ã—ã¦ä¿æŒ
        self.recognition_results = []  # èªè­˜çµæœã‚’ä¿æŒ

    def callback(self, indata, frames, time, status):
        """This is called (from a separate thread) for each audio block."""
        if status:
            print(f'Audio callback error: {status}')
        self.audio_queue.put(indata.copy())

        # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®ãŸã‚ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿åé›†
        self.current_chunk.extend(indata.flatten())
        self.samples_collected += len(indata.flatten())

        # ãƒãƒ£ãƒ³ã‚¯ãŒå®Œæˆã—ãŸã‚‰èªè­˜ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        if self.samples_collected >= CHUNK_SAMPLES:
            chunk_data = np.array(self.current_chunk[:CHUNK_SAMPLES], dtype=np.int16)
            self.recognition_queue.put(chunk_data)
            # æ®‹ã‚Šã®ã‚µãƒ³ãƒ—ãƒ«ã‚’æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã®é–‹å§‹ç‚¹ã¨ã™ã‚‹
            self.current_chunk = self.current_chunk[CHUNK_SAMPLES:]
            self.samples_collected -= CHUNK_SAMPLES

    def start_recording(self):
        self.audio_data = []
        self.current_chunk = []
        self.samples_collected = 0
        self.is_recording = True
        self.recognition_results = []  # èªè­˜çµæœã‚’ãƒªã‚»ãƒƒãƒˆ
        self.stream = sd.InputStream(
            channels=CHANNELS,
            samplerate=SAMPLE_RATE,
            dtype=np.int16,
            callback=self.callback
        )
        self.stream.start()

        # èªè­˜ã‚¹ãƒ¬ãƒƒãƒ‰ã®é–‹å§‹
        self.recognition_thread = threading.Thread(target=self.recognition_worker)
        self.recognition_thread.start()

    def stop_recording(self):
        if hasattr(self, 'stream'):
            self.stream.stop()
            self.stream.close()
        self.is_recording = False

        # æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
        while not self.audio_queue.empty():
            self.audio_data.append(self.audio_queue.get())

        # èªè­˜ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†å¾…ã¡
        if self.recognition_thread:
            self.recognition_queue.put(None)  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
            self.recognition_thread.join()

        return np.concatenate(self.audio_data) if self.audio_data else None

    def recognition_worker(self):
        """éŸ³å£°èªè­˜ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰"""
        try:
            while True:
                chunk = self.recognition_queue.get()
                if chunk is None:  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
                    break

                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=True) as temp_file:
                    save_audio(chunk, temp_file.name)
                    # Whisperã§èªè­˜
                    try:
                        result = self.whisper_model.transcribe(
                            temp_file.name,
                            language='ja',
                            fp16=False
                        )
                        if result["text"].strip():  # ç©ºã§ãªã„çµæœã®ã¿ã‚’è¿½åŠ 
                            self.recognition_results.append(result["text"])
                    except Exception as e:
                        print(f"Recognition error: {e}")
        except Exception as e:
            print(f"Worker thread error: {e}")

def save_audio(audio_data, filename):
    """éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’WAVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹"""
    if isinstance(filename, Path):
        filename = str(filename)
    with wave.open(filename, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(2)  # 16-bit
        wf.setframerate(SAMPLE_RATE)
        wf.writeframes(audio_data.tobytes())

def get_audio_duration(file_path):
    """WAVãƒ•ã‚¡ã‚¤ãƒ«ã®å†ç”Ÿæ™‚é–“ã‚’å–å¾—ã™ã‚‹"""
    with wave.open(str(file_path), 'rb') as wf:
        frames = wf.getnframes()
        rate = wf.getframerate()
        duration = frames / float(rate)
        return round(duration, 1)

def initialize_whisper():
    """Whisperãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
    if 'whisper_model' not in st.session_state:
        with st.spinner('Whisperãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...'):
            st.session_state.whisper_model = load_whisper_model()
            st.session_state.recognition_results = []

def main():
    st.title("éŸ³å£°éŒ²éŸ³ãƒ»èªè­˜ã‚¢ãƒ—ãƒª")

    # Whisperãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    initialize_whisper()

    # AudioRecorderã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
    if 'audio_recorder' not in st.session_state:
        st.session_state.audio_recorder = AudioRecorder(st.session_state.whisper_model)

    # éŒ²éŸ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ç”¨ã®ã‚«ãƒ©ãƒ 
    col1, col2 = st.columns(2)

    with col1:
        # éŒ²éŸ³ãƒœã‚¿ãƒ³
        if not st.session_state.audio_recorder.is_recording:
            if st.button("ğŸ¤ éŒ²éŸ³é–‹å§‹"):
                st.session_state.audio_recorder.start_recording()
                st.rerun()

    with col2:
        # åœæ­¢ãƒœã‚¿ãƒ³
        if st.session_state.audio_recorder.is_recording:
            if st.button("â¹ éŒ²éŸ³åœæ­¢"):
                recorded_audio = st.session_state.audio_recorder.stop_recording()
                if recorded_audio is not None:
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = AUDIO_DIR / f"audio_{timestamp}.wav"
                    # éŸ³å£°ã®ä¿å­˜
                    save_audio(recorded_audio, filename)
                    st.success(f"éŒ²éŸ³ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
                st.rerun()

    # éŒ²éŸ³çŠ¶æ…‹ã¨èªè­˜çµæœã®è¡¨ç¤º
    if st.session_state.audio_recorder.is_recording:
        st.warning("éŒ²éŸ³ä¸­...")

        # èªè­˜çµæœã®è¡¨ç¤ºã‚¨ãƒªã‚¢
        recognition_area = st.empty()
        recognition_text = "\n".join(st.session_state.audio_recorder.recognition_results)
        recognition_area.markdown(f"**èªè­˜çµæœ:**\n{recognition_text}")

    # ä¿å­˜ã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§è¡¨ç¤ºã¨å†ç”Ÿæ©Ÿèƒ½
    st.subheader("éŒ²éŸ³æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«")
    audio_files = [f for f in os.listdir(AUDIO_DIR) if f.endswith('.wav')]
    if audio_files:
        for audio_file in sorted(audio_files, reverse=True):
            col1, col2 = st.columns([3, 1])

            with col1:
                # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±è¡¨ç¤º
                file_path = AUDIO_DIR / audio_file
                duration = get_audio_duration(file_path)
                st.text(f"{audio_file} (éŒ²éŸ³æ™‚é–“: {duration}ç§’)")

                # éŸ³å£°å†ç”Ÿç”¨ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼
                with open(file_path, 'rb') as audio_file:
                    st.audio(audio_file.read(), format='audio/wav')

            with col2:
                # å‰Šé™¤ãƒœã‚¿ãƒ³
                if st.button("ğŸ—‘ï¸ å‰Šé™¤", key=f"delete_{audio_file}"):
                    os.remove(file_path)
                    st.success("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                    st.rerun()
    else:
        st.info("éŒ²éŸ³ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“")

if __name__ == "__main__":
    main()
